{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I import the cleaned movie-review data and tokenize it. Then, I vectorize the reviews using TF-IDF and use the results to train and test a random forest classifier. I consider at the metrics accuracy, precision, recall, and F1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score,\\\n",
    "    recall_score, f1_score\n",
    "\n",
    "from text_to_tokens import load, to_words_text, remove_stop_words_text,\\\n",
    "    stem_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll tokenize each document by paritioning it on whitespace. In addition, I'll remove English stop words and stem all the words that remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm working with the IMDB movie reviews dataset\n",
    "ds_name = 'imdb'\n",
    "docs_all = load(ds_name)\n",
    "\n",
    "# strings needed to specify input/output filenames\n",
    "dir = '../data'\n",
    "tag = 'tokenized'\n",
    "suffix = 'pkl'\n",
    "\n",
    "df_tokenized = {}\n",
    "for (key, docs) in docs_all.items():\n",
    "    new_dict = docs\n",
    "    new_dict = to_words_text(new_dict)\n",
    "    new_dict = remove_stop_words_text(new_dict)\n",
    "    new_dict = stem_text(new_dict)        \n",
    "    df_tokenized[key] = new_dict\n",
    "        \n",
    "    filename = f'{dir}/{ds_name}-{key}-{tag}.{suffix}'\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(df_tokenized[key], f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a test of the above code, I'll read in both pickled files and give them a cursory inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train: (25000, 2)\n",
      "Shape of test: (25000, 2)\n"
     ]
    }
   ],
   "source": [
    "with open(f'{dir}/{ds_name}-train-{tag}.{suffix}', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "\n",
    "with open(f'{dir}/{ds_name}-test-{tag}.{suffix}', 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "print(f'Shape of train: ({len(train[\"text\"])}, {len(train.keys())})')\n",
    "print(f'Shape of test: ({len(test[\"text\"])}, {len(test.keys())})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll print out the first 10 tokens (words) of an arbitrary text sample from each of train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admit',\n",
       " 'laugh',\n",
       " 'watch',\n",
       " 'movi',\n",
       " 'few',\n",
       " 'comedi',\n",
       " 'sawbr',\n",
       " 'br',\n",
       " 'budget',\n",
       " 'have',\n",
       " 'consist']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text'][1000][:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['film',\n",
       " 'about',\n",
       " 'struggl',\n",
       " 'actor',\n",
       " 'tri',\n",
       " 'satisfact',\n",
       " 'life',\n",
       " 'especi',\n",
       " 'love',\n",
       " 'he',\n",
       " 'tast']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['text'][1000][:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll use TF-IDF to vectorize the collection of text tokens to get ready for a classification analysis using `RandomForestClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love scifi am will put lot scifi moviestv usua...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worth entertain valu a rental especi like acti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a total averag film a semialright action seque...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>star rate saturday night friday night friday m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>off let say havent enjoy van damm movi bloodsp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  love scifi am will put lot scifi moviestv usua...      0\n",
       "1  worth entertain valu a rental especi like acti...      0\n",
       "2  a total averag film a semialright action seque...      0\n",
       "3  star rate saturday night friday night friday m...      0\n",
       "4  off let say havent enjoy van damm movi bloodsp...      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For both train and test sets, join the tokens together to form a corpus.\n",
    "train_df = pd.DataFrame(train)\n",
    "train_df['text'] = train_df['text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "test_df = pd.DataFrame(test)\n",
    "test_df['text'] = test_df['text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider a subset of each (train and test) shuffled corpus.\n",
    "n_samples = 7_500\n",
    "\n",
    "train_indices = choice(np.arange(train_df.shape[0]), n_samples)\n",
    "test_indices = choice(np.arange(test_df.shape[0]), n_samples)\n",
    "\n",
    "small_train_df = train_df.loc[train_indices, :] \n",
    "small_test_df = test_df.loc[test_indices, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_ft: (7500, 750)\n",
      "Shape of X_test_t: (7500, 750)\n"
     ]
    }
   ],
   "source": [
    "X_train = small_train_df['text']\n",
    "X_test = small_test_df['text']\n",
    "y_train = small_train_df['label']\n",
    "y_test = small_test_df['label']\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.95,\n",
    "    min_df=2,\n",
    "    max_features=750\n",
    ")\n",
    "\n",
    "X_train_ft = np.array(vectorizer.fit_transform(X_train).todense())\n",
    "X_test_t = np.array(vectorizer.transform(X_test).todense())\n",
    "\n",
    "print(f'Shape of X_train_ft: {X_train_ft.shape}')\n",
    "print(f'Shape of X_test_t: {X_test_t.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "Best Parameters: {'max_depth': 7, 'n_estimators': 300}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 6, 7]\n",
    "}\n",
    "\n",
    "search = GridSearchCV(\n",
    "    rfc,\n",
    "    param_grid=param_grid,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "results = search.fit(X_train_ft, y_train)\n",
    "print(f'Best Parameters: {results.best_params_}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scoring the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training scores:\n",
      "Accuracy: 0.868\n",
      "Precision: 0.83\n",
      "Recall: 0.922\n",
      "F1: 0.874\n",
      "\n",
      "Test scores:\n",
      "Accuracy: 0.799\n",
      "Precision: 0.775\n",
      "Recall: 0.834\n",
      "F1: 0.803\n"
     ]
    }
   ],
   "source": [
    "model = results.best_estimator_\n",
    "y_train_pred = model.predict(X_train_ft)\n",
    "y_test_pred = model.predict(X_test_t)\n",
    "\n",
    "print('Training scores:')\n",
    "print(f'Accuracy: {round(accuracy_score(y_train, y_train_pred), 3)}')\n",
    "print(f'Precision: {round(precision_score(y_train, y_train_pred), 3)}')\n",
    "print(f'Recall: {round(recall_score(y_train, y_train_pred), 3)}')\n",
    "print(f'F1: {round(f1_score(y_train, y_train_pred), 3)}\\n')\n",
    "\n",
    "print('Test scores:')\n",
    "print(f'Accuracy: {round(accuracy_score(y_test, y_test_pred), 3)}')\n",
    "print(f'Precision: {round(precision_score(y_test, y_test_pred), 3)}')\n",
    "print(f'Recall: {round(recall_score(y_test, y_test_pred), 3)}')\n",
    "print(f'F1: {round(f1_score(y_test, y_test_pred), 3)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 7,500 samples and 750 features, I obtained the above moderately successful training and test scores. Generally, the training scores are significantly higher than the test scores, indicative of overfitting. These calculations were performed on a PC with 16 GB RAM and no GPU support. Perhaps the results would improve if the random forest classifier were trained on the full 25,000 sample dataset with a higher number of features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../models/random-forest-classifier/rfc.pkl'\n",
    "with open(path, 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
